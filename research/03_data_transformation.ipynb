{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\Customer Churn Prediction with MLOps\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\Customer Churn Prediction with MLOps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from mlProject import logger\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlProject import logger\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\n",
    "    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\n",
    "\n",
    "    # I am only adding train_test_spliting cz this data is already cleaned up\n",
    "\n",
    "\n",
    "    def transform_data(self, data):\n",
    "        \"\"\"Applies preprocessing transformations to the dataset.\"\"\"\n",
    "        \n",
    "        # Convert TotalCharges to numeric\n",
    "        data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "\n",
    "        # Define target and features\n",
    "        X = data.drop(columns=['customerID', 'Churn'])  # Drop non-useful and target column\n",
    "        y = data['Churn'].map({'No': 0, 'Yes': 1})  # Convert Churn to 0 & 1\n",
    "\n",
    "        # imputer = SimpleImputer(strategy='most_frequent')  # Fill NaN with the most frequent value (0 or 1)\n",
    "        # y = imputer.fit_transform(y.values.reshape(-1, 1)).ravel().astype(int)  # Ensure y is an integer\n",
    "        \n",
    "\n",
    "\n",
    "        # Define column categories\n",
    "        numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "        binary_categorical_features = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
    "        multi_categorical_features = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                                      'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "                                      'Contract', 'PaymentMethod']\n",
    "\n",
    "        # Define preprocessing steps\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),  # Fill missing values\n",
    "            ('scaler', StandardScaler())  # Standardize numerical data\n",
    "        ])\n",
    "\n",
    "        binary_transformer = Pipeline(steps=[\n",
    "            ('encoder', OneHotEncoder(drop='if_binary', dtype=int))  # Convert binary to 0 & 1\n",
    "        ])\n",
    "\n",
    "        multi_transformer = Pipeline(steps=[\n",
    "            ('encoder', OneHotEncoder(drop='first', dtype=int))  # One-Hot Encode multi-category columns\n",
    "        ])\n",
    "\n",
    "        # Combine all transformations\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('bin', binary_transformer, binary_categorical_features),\n",
    "            ('multi', multi_transformer, multi_categorical_features)\n",
    "        ])\n",
    "\n",
    "        # Apply transformations\n",
    "        X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "        # Convert transformed data into a DataFrame\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "\n",
    "        return X_transformed_df, y\n",
    "\n",
    "    def train_test_splitting(self):\n",
    "        \"\"\"Splits the dataset into train and test sets after transformation.\"\"\"\n",
    "        # Load raw data\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        # Apply transformations\n",
    "        X_transformed, y = self.transform_data(data)\n",
    "\n",
    "        # Train-test split (75% train, 25% test)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.25, random_state=42)\n",
    "\n",
    "        # y_train = pd.Series(y_train)\n",
    "        # y_test = pd.Series(y_test)\n",
    "\n",
    "        # y_train = pd.Series(y_train).reset_index(drop=True)\n",
    "        # y_test = pd.Series(y_test).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # Save transformed train and test data\n",
    "        train_data = pd.concat([X_train, y_train], axis=1)\n",
    "        test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "        train_path = os.path.join(self.config.root_dir, \"train.csv\")\n",
    "        test_path = os.path.join(self.config.root_dir, \"test.csv\")\n",
    "\n",
    "        train_data.to_csv(train_path, index=False)\n",
    "        test_data.to_csv(test_path, index=False)\n",
    "\n",
    "        logger.info(\"Data transformed and split into training and test sets\")\n",
    "        logger.info(f\"Train shape: {train_data.shape}\")\n",
    "        logger.info(f\"Test shape: {test_data.shape}\")\n",
    "\n",
    "        print(\"Train shape:\", train_data.shape)\n",
    "        print(\"Test shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-06 15:30:05,761: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-02-06 15:30:05,762: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-02-06 15:30:05,765: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-02-06 15:30:05,766: INFO: common: created directory at: artifacts]\n",
      "[2025-02-06 15:30:05,766: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-02-06 15:30:05,913: INFO: 3404203253: Data transformed and split into training and test sets]\n",
      "[2025-02-06 15:30:05,914: INFO: 3404203253: Train shape: (5282, 30)]\n",
      "[2025-02-06 15:30:05,914: INFO: 3404203253: Test shape: (1761, 30)]\n",
      "Train shape: (5282, 30)\n",
      "Test shape: (1761, 30)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.train_test_splitting()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train shape: (6621, 30)\n",
    "Test shape: (3047, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
